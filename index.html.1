<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0041)https://people.eecs.berkeley.edu/~barron/ -->
<!-- WW91IHdvbiwgZ2V0IHlvdXJzZWxmIHNvbWV0aGluZyBuaWNlCg== -->
<html><head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #07889b; /*#1772d0;*/
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #e37222; /*#f7b733;*/ /*f09228;*/
    text-decoration:none;
    }
    body,td,th,tr,p {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 15px; /*14*/
    }
    a {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    }
    strong {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 15px; /*14*/
    }
    heading {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 22px;
    color: #e37222; /*#fc4a1a;*/
    }
    heading2 {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 18px;
    }
    papertitle {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 15px; /*14*/
    font-weight: 700;
    }
    name {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 42px;
    }
    li:not(:last-child) {
        margin-bottom: 5px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="">
  <title>Malinovskii Vladimir, ML researcher</title>

  <style>
    /* latin-ext */
    @font-face {
    font-family: 'Lato';
    font-style: normal;
    font-weight: 400;
    src: local('Lato Regular'), local('Lato-Regular'), url(http://fonts.gstatic.com/s/lato/v11/8qcEw_nrk_5HEcCpYdJu8BTbgVql8nDJpwnrE27mub0.woff2) format('woff2');
    unicode-range: U+0100-024F, U+1E00-1EFF, U+20A0-20AB, U+20AD-20CF, U+2C60-2C7F, U+A720-A7FF;
    }
    /* latin */
    @font-face {
    font-family: 'Lato';
    font-style: normal;
    font-weight: 400;
    src: local('Lato Regular'), local('Lato-Regular'), url(http://fonts.gstatic.com/s/lato/v11/MDadn8DQ_3oT6kvnUq_2r_esZW2xOQ-xsNqO47m55DA.woff2) format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
    }
    /* latin-ext */
    @font-face {
    font-family: 'Lato';
    font-style: normal;
    font-weight: 700;
    src: local('Lato Bold'), local('Lato-Bold'), url(http://fonts.gstatic.com/s/lato/v11/rZPI2gHXi8zxUjnybc2ZQFKPGs1ZzpMvnHX-7fPOuAc.woff2) format('woff2');
    unicode-range: U+0100-024F, U+1E00-1EFF, U+20A0-20AB, U+20AD-20CF, U+2C60-2C7F, U+A720-A7FF;
    }
    /* latin */
    @font-face {
    font-family: 'Lato';
    font-style: normal;
    font-weight: 700;
    src: local('Lato Bold'), local('Lato-Bold'), url(http://fonts.gstatic.com/s/lato/v11/MgNNr5y1C_tIEuLEmicLmwLUuEpTyoUstqEm5AMlJo4.woff2) format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
    }
    /* latin-ext */
    @font-face {
    font-family: 'Lato';
    font-style: italic;
    font-weight: 400;
    src: local('Lato Italic'), local('Lato-Italic'), url(http://fonts.gstatic.com/s/lato/v11/cT2GN3KRBUX69GVJ2b2hxn-_kf6ByYO6CLYdB4HQE-Y.woff2) format('woff2');
    unicode-range: U+0100-024F, U+1E00-1EFF, U+20A0-20AB, U+20AD-20CF, U+2C60-2C7F, U+A720-A7FF;
    }
    /* latin */
    @font-face {
    font-family: 'Lato';
    font-style: italic;
    font-weight: 400;
    src: local('Lato Italic'), local('Lato-Italic'), url(http://fonts.gstatic.com/s/lato/v11/1KWMyx7m-L0fkQGwYhWwuuvvDin1pK8aKteLpeZ5c0A.woff2) format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
    }
    /* latin-ext */
    @font-face {
    font-family: 'Lato';
    font-style: italic;
    font-weight: 700;
    src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(http://fonts.gstatic.com/s/lato/v11/AcvTq8Q0lyKKNxRlL28Rn4X0hVgzZQUfRDuZrPvH3D8.woff2) format('woff2');
    unicode-range: U+0100-024F, U+1E00-1EFF, U+20A0-20AB, U+20AD-20CF, U+2C60-2C7F, U+A720-A7FF;
    }
    /* latin */
    @font-face {
    font-family: 'Lato';
    font-style: italic;
    font-weight: 700;
    src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(http://fonts.gstatic.com/s/lato/v11/HkF_qI1x_noxlxhrhMQYEJBw1xU1rKptJj_0jans920.woff2) format('woff2');
    unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
    }
    @font-face {
    font-family: 'TeXGyrePagella';
    src: url(cc7fb5f529b178f4f999035667192117.woff) format('woff'),
         url(53f87e2b745c3930f4a684e34484dd2e.eot),
         url(53f87e2b745c3930f4a684e34484dd2e.eot?#iefix) format('embedded-opentype'),
         url(43fbbf49ee17492cfa72cb084ea73340.ttf) format('truetype'),
         url(986f93ad70b359de2d69943f39bae648.svg#tex_gyre_pagellaregular) format('svg');
    font-weight: normal;
    font-style: normal;
    }

    @font-face {
    font-family: 'TeXGyrePagella';
    src: url(58a03c9f1791931cda82b51a1f384f54.woff) format('woff'),
         url(d80ec4c0756e6a991db5be57d0d41ef7.eot),
         url(d80ec4c0756e6a991db5be57d0d41ef7.eot?#iefix) format('embedded-opentype'),
         url(7c60593fb68ff752eb7e5cc4c906aef2.ttf) format('truetype'),
         url(0cdd340430bed86ce28565888c601e17.svg#tex_gyre_pagellabold) format('svg');
    font-weight: bold;
    font-style: normal;
    }

    @font-face {
    font-family: 'TeXGyrePagella';
    src: url(fef23d0916ebdad441bc284a3fcedfc4.woff) format('woff'),
         url(211d57d91375c721e71c51337a8f0bce.eot),
         url(211d57d91375c721e71c51337a8f0bce.eot?#iefix) format('embedded-opentype'),
         url(c070c775e677e792d4b116f52e8597d9.ttf) format('truetype'),
         url(043aea05e94b28b2e3022058acb219fb.svg#tex_gyre_pagellaitalic) format('svg');
    font-weight: normal;
    font-style: italic;

    }

    @font-face {
    font-family: 'TeXGyrePagella';
    src: url(e2c466052f3f95c4a14c05fd705fff5b.woff) format('woff'),
         url(c3100698a9bf9f27de6ef9478ff4a0d2.eot),
         url(c3100698a9bf9f27de6ef9478ff4a0d2.eot?#iefix) format('embedded-opentype'),
         url(5f9bab8ea372719d0f0cf5b13ec588ff.ttf) format('truetype'),
         url(081b590739d6c031d8b092a9ef9fdcf8.svg#tex_gyre_pagellabold_italic) format('svg');
    font-weight: bold;
    font-style: italic;

    }
    /*
    *  Academicons 1.8.0 by James Walsh - https://github.com/jpswalsh
    *  Fonts generated using the IcoMoon app - http://icomoon.io/app
    *  Square icons designed to be used alongside Font Awesome square icons - https://fortawesome.github.io/Font-Awesome/
    *  Licenses - Font: SIL OFL 1.1, CSS: MIT License
    */
    @font-face {
    font-family: 'Academicons';
    src:url(c812316ebc65d292dd83f7d2cc2726bf.eot);
    src:url(c812316ebc65d292dd83f7d2cc2726bf.eot) format('embedded-opentype'),
        url(0575fa935075812534afa2805352ec54.ttf) format('truetype'),
        url(64794d6b599c274edab750798fd89a5c.woff) format('woff'),
        url(cfe4fb274f02c9b99412b2f02b9338bb.svg#academicons) format('svg');
    font-weight: normal;
    font-style: normal;
    }
  </style>

  <style id="dark-reader-style" type="text/css">@media screen {

/* Leading rule */
/*html {
  -webkit-filter: brightness(100%) contrast(100%) grayscale(20%) sepia(10%) !important;
}*/

/* Text contrast */
html {
  text-shadow: 0 0 0 !important;
}

/* Full screen */
*:-webkit-full-screen, *:-webkit-full-screen * {
  -webkit-filter: none !important;
}

/* Page background */
html {
  background: rgb(255,255,255) !important;
}

}</style>

<script type="text/javascript">
   function visibility_on(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'none')
            e.style.display = 'block';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'none')
            e.style.display = 'block';
   }
   function visibility_off(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'block')
            e.style.display = 'none';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'block')
            e.style.display = 'none';
   }
   function toggle_visibility(id) {
       var e = document.getElementById(id+"_text");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
       var e = document.getElementById(id+"_img");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
   }
   function toggle_vis(id) {
       var e = document.getElementById(id);
       if (e.style.display == 'none')
           e.style.display = 'inline';
       else
           e.style.display = 'none';
   }
</script>

</head>
</div>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="67%" valign="middle">
        <p align="center">
        <name>Malinovskii Vladimir</name><br>
        galqiwi at gmail dot com
        </p>
        <p>I am an Assistant Professor in <a href="http://cs.stanford.edu/">Computer Science</a> and <a href="https://ee.stanford.edu/">Electrical Engineering</a> at <a href="https://stanford.edu">Stanford University</a> and co-founder of <a href="https://physicalintelligence.company/">Pi</a>. My lab, <a href="https://irislab.stanford.edu/">IRIS</a>, studies intelligence through robotic interaction at scale, and is affiliated with <a href="http://ai.stanford.edu/">SAIL</a> and the <a href="http://ml.stanford.edu/">ML Group</a>.
        </p>
        <p>
        <i>I am interested in the capability of robots and other agents to develop broadly intelligent behavior through learning and interaction.</i>
        </p>
        <p>
        Previously, I completed my Ph.D. in computer science at <a href="https://eecs.berkeley.edu/">UC Berkeley</a> and my B.S. in electrical engineering and computer science at <a href="https://mit.edu">MIT</a>. I also spent time at Google as part of the <a href="https://ai.google/research/teams/brain/">Google Brain</a> team.
        </p>

        <b>Prospective students and post-docs</b>, please see <a href="https://irislab.stanford.edu/contact.html">this page</a>.
        </p>


        <p align="center">
<a href="_files/cv.pdf">CV</a> &nbsp;/&nbsp;
<a href="http://ai.stanford.edu/~cbfinn/bio.txt">Bio</a> &nbsp;/&nbsp;
<a href="_files/dissertation.pdf">PhD Thesis</a> &nbsp;/&nbsp;
<a href="https://scholar.google.com/citations?user=vfPE6hgAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
<!-- <a href="http://www.github.com/cbfinn/"> GitHub </a> &nbsp;/&nbsp;-->
<a href="https://www.twitter.com/chelseabfinn/">Twitter</a> &nbsp;/&nbsp;
<a href="https://irislab.stanford.edu/">IRIS Lab</a>
        </p>
        </td>
        <td width="33%">
        <img width="280" src="./_files/sail_headshot_left_facing_crop.jpg">
        <!--img height="280" width="280" src="./_files/ChelseaFinn_hires.jpg"-->
        </td>
      </tr>
  </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr><td>
            <heading>News</heading>
            <p>See our <a href="https://irislab.stanford.edu/">lab website</a> for up-to-date news.</p>
            <ul>
                <li>Our work on meta-learning for giving feedback to students was <a href="https://www.nytimes.com/2021/07/20/technology/ai-education-neural-networks.html">featured in the New York Times</a>. See the <a href="https://ai.stanford.edu/blog/prototransformer/">blog post</a>, <a href="https://arxiv.org/abs/2107.14035">paper</a>, and <a href="_files/acl_meta_edu.pdf">slides</a> for more information.</li>
                <li>I am honored to have received <a href="http://intel.com/content/www/us/en/research/blogs/intel-rising-stars-awards-2020.html">Intel's Rising Star Faculty Award</a> and the <a href="https://www.microsoft.com/en-us/research/blog/microsoft-research-faculty-fellowship-2020-researchers-in-academia-at-the-forefront-of-technology/">Microsoft Faculty Fellowship award</a>.</li>
                <li>In Fall 2019, I taught a <a href="http://web.stanford.edu/class/cs330/">new course</a> on deep multi-task and meta learning. Lecture videos are available <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5">here</a>.</li>
                <li>I am honored and thrilled to have received the <a href="https://awards.acm.org/about/2018-doctoral-dissertation">ACM 2018 Doctoral Dissertation Award</a> for my thesis, <a href="_files/dissertation.pdf">Learning to Learn with Gradients</a>.</li>
                <li>Our work on enabling robots to use objects as tools was featured in <a href="https://www.technologyreview.com/s/613304/a-robot-has-figured-out-how-to-use-tools/">an article by MIT Technology Review</a></li>
                <li>I am honored to have received the <a href="https://www.technologyreview.com/lists/innovators-under-35/2018/pioneer/chelsea-finn/">MIT TR35 pioneer award</a>.</li>
                <li>In August 2018, I completed my PhD!</li>
              <li> <a href="javascript:toggle_vis('news')">show more</a> </li>
              <div id="news" style="display:none"> 
                <li>Some of my research was featured in a <a href="https://techcrunch.com/video/teaching-robots-to-learn-how-to-learn/">TechCrunch article</a>.</li>
              <li> At <a href="html://nips.cc">NIPS 2017</a>, we showcased our research on meta-imitation learning and visual foresight in a live robot demo! For more information and a video, see <a href="http://rail.eecs.berkeley.edu/nips_demo.html">this page</a>.</li>
              <li> In summer 2017, I co-organized <a href="http://people.eecs.berkeley.edu/~anca/papers/BAIRCampFlyer.pdf">BAIR camp</a>, a 2-day summer camp on human-centered AI for high-school students from low income backgrounds. We are organizing <a href="http://bair.berkeley.edu/ai4all/">a second camp</a> in August 2018.</li>
              <li> In Spring 2017, I helped develop and co-taught a <a href="http://rll.berkeley.edu/deeprlcoursesp17/">course on deep reinforcement learning</a>.</li>
              <li>I co-organized a workshop at NIPS 2016 on <a href="https://sites.google.com/site/nips16interaction/">Deep Learning for Action and Interaction</a> (<a href="https://www.youtube.com/watch?v=vTgwWobuoFw&list=PL_iWQOsE6TfVCLmikLdaQOBntJuCZLwQY&index=1">videos here</a>). </li>
              <li>My colleagues and I have released the robotic grasping and pushing data used in Levine et al. '16 (ISER) and Finn et al. '16 (NIPS): <a href="https://sites.google.com/site/brainrobotdata">Google Brain Robotics Data</a>.</li>
              </div>
            </ul>
        </td></tr>


        <!--
        <tr><td>
            <heading>Blog Posts</heading>
            <ul>
                <li> <a href="https://ai.stanford.edu/blog/prototransformer/">Meta-Learning Student Feedback to 16,000 Solutions</a>: our work on studying meta-learning for education and how we can scale student feedback.</li>
                <li> <a href="http://ai.stanford.edu/blog/robonet/">RoboNet: A Dataset for Large-Scale Multi-Robot Learning</a>: our work on accumulating and sharing data across robotics labs for broad generalization.</li>
                <li> <a href="https://bair.berkeley.edu/blog/2019/05/28/end-to-end/">End-to-End Deep Reinforcement Learning without Reward Engineering</a>: how robots can learn skills end-to-end from pixels without reward engineering.</li>
                <li> <a href="https://bair.berkeley.edu/blog/2019/04/11/tools/">Robots that Learn to Use Improvised Tools</a>: how robots can figure out how to solve tasks using tools, including unconventional tools, by learning from a combination of unsupervised interaction and example demonstrations.</li>
              <li> <a href="http://bair.berkeley.edu/blog/2018/11/30/visual-rl/">Visual Model-Based Reinforcement Learning as a Path towards Generalist Robots</a>: our work on learning a single model that can be used to accomplish many different tasks, without human supervision. </li>
              <li> <a href="javascript:toggle_vis('blogs')">show more</a> </li>
              <div id="blogs" style="display:none"> 
              <li> <a href="https://bair.berkeley.edu/blog/2019/03/21/tactile/">Manipulation By Feel</a>: how we can use visual foresight to enable robots to manipulate objects entirely by feel without vision.</li>
              <li> <a href="http://bair.berkeley.edu/blog/2018/06/28/daml/">One-Shot Imitation from Watching Videos</a>: describes our work on enabling robots to learn to manipulate new objects by watching humans.</li>
              <li> <a href="http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/">Learning to Learn</a>: covers recent approaches to meta-learning and our recent paper on model-agnostic meta-learning. It was also <a href="https://www.jiqizhixin.com/articles/47f6d636-281d-4cbe-a0cc-9d3670fcdc64">translated into Chinese</a>.</li>
              <li><a href="https://research.googleblog.com/2016/10/how-robots-can-acquire-new-skills-from.html">How Robots Can Acquire New Skills from Their Shared Experience</a>:
              features some work done by me and my colleagues at Google Brain, X, and DeepMind on learning across multiple robots. It was nicely summarized
              by the MIT Technology Review <a href="https://www.technologyreview.com/s/602529/google-builds-a-robotic-hive-mind-kindergarten/">here</a>. </li>
             </div>
            </ul>
        </td></tr>
        -->


        <tr><td>
            <!--<heading>Recent Talk (October 2018)</heading><br><br>-->
             <!--heading>Recent Talk (December 2019)</heading><br><br-->
             <!--heading>Recent Talk (June 2020)</heading><br><br>-->
             <heading>Recent Talks</heading><br><br>

             <b>Robotics Focused Talk (November 2022)</b>
             <iframe width="780" height="439" src="https://www.youtube.com/embed/m8pQeXe7J0w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
             <br><br>

             <b>Machine Learning Focused Talk (June 2022)</b>
             <iframe width="780" height="439" src="https://www.youtube.com/embed/gtDo9njJKb8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
             <!--iframe width="780" height="439" src="https://www.youtube.com/embed/wv1zXnxRCCM?start=639" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe-->
        <!--iframe width="780" height="439" src="https://www.youtube.com/embed/tcFLwgqO7G0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe-->
        <!--<iframe width="780" height="439" src="https://www.youtube.com/embed/i05Fk4ebMY0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
        </td></tr>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <heading>Students and Post-Docs</heading>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <td width="75%" valign="center">

          <p>
          See <a href="https://irislab.stanford.edu/people.html">this page</a> for a list of lab members. 
        </p>
        </td>
      </tr>
      </tbody></table>




      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <heading>Teaching</heading>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <!--<td width="25%"><img src="./_files/teach_crop.jpg" alt="teach" width="160" height="160"></td>-->
        <td width="75%" valign="center">

            <p>
            <a href="https://cs224r.stanford.edu">
                <papertitle>Stanford CS224R: Deep Reinforcement Learning</papertitle></a> - Spring 2023<br>

        <a href="https://cs330.stanford.edu/">
            <papertitle>Stanford CS330: Deep Multi-Task and Meta Learning</papertitle></a> - Fall 2019, Fall 2020, Fall 2021, Fall 2022<br>

          <a href="https://stanford-cs221.github.io/spring2021/">
              <papertitle>Stanford CS221: Artificial Intelligence: Principles and Techniques</papertitle></a> - Spring 2020, Spring 2021 <br>

        <a href="http://rll.berkeley.edu/deeprlcoursesp17/">
            <papertitle>UCB CS294-112: Deep Reinforcement Learning</papertitle></a> - Spring 2017<br>

        </p>
        </td>
      </tr>
      </tbody></table>


        <tr><td>
            <heading>Tutorials and Lectures</heading>
            <ul>
                <li>Lecture videos for the Fall 2021 edition of CS330 are available online <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rMIJ-TvblAIkw28Wxi27B36">here</a>.</li>
                <li>In Fall 2019, I taught a <a href="http://web.stanford.edu/class/cs330/">new course</a> on deep multi-task and meta learning. Lecture videos are available <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5">here</a>.</li>
                <li>At ICML 2019 and CVPR 2019, I gave an invited tutorial on Meta-Learning: from Few-Shot Learning to Rapid Reinforcement Learning. Slides, video, and references are linked <a href="https://sites.google.com/view/icml19metalearning">here</a>.
                <li>In December 2018, I gave a tutorial on model-based reinforcement learning at the CIFAR LMB program meeting (<a href="_files/mbrl_cifar.pdf">slides here</a>).</li>
              <li> At <a href="http://icml.cc">ICML 2017</a>, I gave a <a href="https://2017.icml.cc/Conferences/2017/Tutorials">tutorial</a> with Sergey Levine on Deep Reinforcement Learning, Decision Making, and Control (<a href = "https://sites.google.com/view/icml17deeprl">slides here</a>, <a href="https://vimeo.com/240428644">video here</a>).</li>
              <li> In August 2017, I gave guest lectures on model-based reinforcement learning and inverse reinforcement learning at the <a href="https://www.deepbootcamp.io/">Deep RL Bootcamp</a> (slides <a href="_files/mbrl_bootcamp.pdf">here</a> and <a href="_files/bootcamp_inverserl.pdf">here</a>, videos <a href="https://youtu.be/iC2a7M9voYU">here</a> and <a href="https://youtu.be/d9DlQSJQAoI">here</a>). </li>
              <li> In Spring 2017, I co-taught a course on deep reinforcement learning at UC Berkeley. All lecture video and slides are available <a href="http://rll.berkeley.edu/deeprlcoursesp17/#lecture-videos">here</a>.</li>
            </ul>
        </td></tr>

        <!--
        <tr><td>
            <heading>Invited Talks</heading>
            <ul>
                <li>I gave a talk on meta-learning for giving feedback to students (<a href="_files/acl_meta_edu.pdf">slides here</a>) at the <a href="https://meta-nlp-2021.github.io/">ACL 2021 MetaNLP workshop</a>.</li>
                <li>I gave a talk on meta-learning (<a href="_files/samsung_ai_forum.pdf">slides here</a>, <a href="https://youtu.be/AhvFggX2xow?t=3379">video here</a>) at the Samsung AI Forum in 2020.</li>
                <li>I gave a talk on data scalability in robot learning (<a href="https://youtu.be/VY4oaRIkzqI?t=9134">video here</a>) at the RSS 2020 Workshop on Self-Supervised Robot Learning.</li>
                <li>At <a href="http://l4dc.org/">L4DC 2020</a>, I gave an invited talk on extrapolation via adaptation (<a href="https://youtu.be/f97AdxsAeJk?t=12900">video here</a>).</li>
                <li>I gave a talk on challenges in multi-task learning and meta-learning (<a href="_files/ias_slides.pdf">slides here</a>, <a href="https://video.ias.edu/workshop/2020/0416-ChelseaFinn">video here</a>) at the IAS Workshop on New Directions in Optimization Statistics and Machine Learning.</li>
                <li>At NeurIPS 2019, I gave an invited talk on Meta-Learning and Memorization (<a href="_files/neurips19_memorization.pdf">slides here</a>, <a href="https://slideslive.com/38921876/bayesian-deep-learning-3">video here</a>) at the Bayesian Deep Learning Workshop</li>
                <li>At RLDM 2019, I gave an invited talk on Reinforcement Learning for Robots (<a href="_files/rldm2019_rl_for_robots.pdf">slides here</a>).</li>
                <li>At CVSS 2019, I gave an invited lecture on Deep Visuomotor Learning (<a href="_files/cvss2019.pdf">slides here</a>).</li>
                <li> <a href="javascript:toggle_vis('talks')">show more</a> </li>
              <div id="talks" style="display:none"> 
                <li>At RSS 2019, I gave invited talks in the workshops on Simulation to Real World Transfer (<a href="_files/rss19_sim2real.pdf">slides here</a>), the workshop on Task-Informed Grasping (<a href="_files/rss19_tig.pdf">slides here</a>), and the workshop on Women in Robotics (<a href="_files/rss19_women.pdf">slides here</a>) </li>
                <li>At ICLR 2019, I gave invited talks at the Task-Agnostic RL Workshop (<a href="_files/tarl_iclr19.pdf">slides here</a>, <a href="https://slideslive.com/38915490/r09-taskagnostic-reinforcement-learning">video here</a>), the workshop on Learning from Limited Labeled Data (<a href="_files/lld_iclr19.pdf">slides here</a>, <a href="https://slideslive.com/38915478/r01-the-2nd-learning-from-limited-labeled-data-lld-workshop-representation-learning-for-weak-supervision-and-beyond">video here</a>).</li>  
                <li>In May 2019, I gave a talk at the <a href="https://www.grasp.upenn.edu/events/chelsea-finn">GRASP Seminar</a> at University of Pennsylvania (<a href="_files/upenn_may2019.pdf">slides here</a>).</li>
                <li>At NeurIPS 2018, I gave invited talks at the Continual Learning Workshop (<a href="_files/neurips18_continual_25min.pdf">slides here</a>), the workshop on Learning to Model the Physical World (<a href="_files/neurips18_model_the_world_25min.pdf">slides here</a>), and the workshop on Spatiotemporal Modeling (<a href="_files/neurips18_spatiotemporal_25min.pdf">slides here</a>).</li>
                <li>In September 2018, I gave a 3-minute talk at EmTech (<a href="https://events.technologyreview.com/video/watch/chelsea-finn-uc-berkeley-innovator">video here</a>) </li>
                <li>In July 2018, I gave a talk at Google DeepMind with Sergey Levine on meta-learning frontiers. (<a href="_files/metalearning_frontiers_2018_small.pdf">slides here</a>)</li>
                <li>At <a href="http://icml.cc">ICML 2018</a>, I gave an invited talk in the <a href="https://sites.google.com/site/automl2018icml/">AutoML workshop</a> (<a href="_files/icml2018_automl_35min.pdf">slides here</a>) and contributed talks in the workshop on <a href="https://sites.google.com/view/goalsrl/">Goal Specifications for RL</a> (<a href="_files/icml2018_goalsrl_metairl8min.pdf">slides here</a>) and the workshop on <a
                        href="https://sites.google.com/view/tadgm/home">Theoretical Foundations and Applications of Deep Generative Models</a> (<a href="_files/icml2018_pgm_platipus.pdf">slides here</a>).</li>
                <li> I gave a lecture on deep visuomotor learning at <a href="http://iplab.dmi.unict.it/icvss2018/">ICVSS 2018</a> (<a href="_files/icvss2018_visuomotor_learning.pdf">slides here</a>).
                <li>At <a href="http://www.roboticsconference.org/">RSS 2018</a>, I gave an invited talk in the <a href="https://sites.google.com/view/learningfromdemonstrations/">Workshop on Learning from Demonstrations for High Level Robotics Tasks</a> (<a href="_files/rss2018_25min.pdf">slides here</a>).</li>
              <li> At <a href="html://nips.cc">NIPS 2017</a>, I gave an invited talk in the <a href="http://metalearning.ml/">Workshop on Meta-Learning</a> (<a href="_files/nips2017_metaworkshop.pdf">slides here</a>) and contributed talks at the <a href="https://sites.google.com/view/deeprl-symposium-nips2017/">Deep Reinforcement Learning Symposium</a> and the workshop on <a href="https://ludwigschmidt.github.io/nips17-dl-workshop-website/">Deep Learning: Bridging Theory and Practice</a>.</li>
              <li> At <a href="http://icml.cc">ICML 2017</a>, I gave an invited talk in the <a href="https://rllabmcgill.github.io/icml2017-rlworkshop/">Reinforcement Learning Workshop</a> (<a href="_files/icml2017rlworkshop.pdf">slides here</a>) and a contributed talk in the <a href="http://rlabstraction2016.wixsite.com/icml-2017">Lifelong Learning Workshop</a> (<a href="_files/icml2017_llworkshop.pdf">slides here</a>).</li>
              <li> At <a href="http://www.roboticsconference.org/">RSS 2017</a>, I gave an invited talk at the workshop on <a href="http://juxi.net/workshop/deep-learning-rss-2017/">New Fronteirs for Deep Learning in Robotics</a> (<a href="_files/rss_dl_workshop_finn.pdf">slides here</a>).</li>
              <li> In May 2017, I gave a talk at the <a href="https://www.eventbrite.com/e/sorl-2017-symposium-on-robot-learning-tickets-32276345418?aff=eac2">Symposium on Robot Learning</a> at UC Berkeley (<a href="_files/learning_through_interaction.pdf">slides here</a>, <a href="https://www.youtube.com/watch?v=Ko8IBbYjdq8&index=5&list=PLYTiwx6hV33tczJmSONGzLMY_CVEiXalH">video here</a>).</li>
              <li> In March 2017, I gave a talk on the <a href="http://rll.berkeley.edu/gps/">guided policy search codebase</a> at the <a href="http://oss4dm.org/">Open Source Software for Decision Making Workshop</a> at Stanford (<a href="https://www.youtube.com/watch?v=eEa66TbYsV0&feature=youtu.be">video here</a>). </li>
              <li> In January 2017, I gave a talk at the Rework Deep Learning Summit in SF (<a href="http://videos.re-work.co/videos/286-end-to-end-deep-robotic-learning">video here</a>). </li>
              <li> At <a href="http://nips.cc">NIPS 2016</a>, I gave invited talks in the <a href="https://sites.google.com/site/nips2016deeplearnings/home">Deep Learning Symposium</a>, <a href="https://sites.google.com/site/deeprlnips2016/">Deep RL Workshop</a>, and <a href="https://rueckert.lima-city.de/NIPSWS2016/WebContent/">Neurorobotics Workshop</a>. I will also be giving a contributed talk in the <a href="http://phys.csail.mit.edu/index.html">Intuitive Physics Workshop</a>.</li>
              </div>
            </ul>
        </td></tr>
        -->

        <!--
        <td width="100%" valign="middle">
            <heading>Selected Publications (<a href="https://irislab.stanford.edu/publications.html">See all</a>)</heading> <br><br>



            <div onmouseover="document.getElementById('tag').style.display = 'block';"
                onmouseout="document.getElementById('tag').style.display='none';">
              <a href="https://arxiv.org/pdf/2109.04617">
                <papertitle>Efficiently Identifying Task Groupings for Multi-Task Learning</papertitle></a><br>
            <a href="https://www.linkedin.com/in/christopher-fifty/">Christopher Fifty</a>,
              <a href="https://scholar.google.com/citations?user=F6omR3gAAAAJ&hl=en">Ehsan Amid</a>, 
              <a href="http://www-personal.umich.edu/~zhezhao/">Zhe Zhao</a>,
              <a href="https://cs.stanford.edu/~tianheyu/">Tianhe Yu</a>,
              <a href="https://scholar.google.com/citations?user=Mv71-IcAAAAJ/">Rohan Anil</a>,
              <i>Chelsea Finn</i><br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2021 <font color="#e37222"><strong>(Spotlight)</strong></font> <br>
              <a href="https://arxiv.org/abs/2109.04617">arXiv</a>  / <a href="https://github.com/google-research/google-research/tree/master/tag">code</a>
            </div>
            <div id="tag" style="display:none">  
              </div><br>

            <div onmouseover="document.getElementById('embr').style.display = 'block';"
                onmouseout="document.getElementById('embr').style.display='none';">
              <a href="https://arxiv.org/pdf/2109.10312">
                <papertitle>Example-Driven Model-Based Reinforcement Learning for Solving Long-Horizon Visuomotor Tasks</papertitle></a><br>
            <a href="https://bohanwu.github.io/">Bohan Wu</a>,
              <a href="http://surajnair.com">Suraj Nair</a>, 
              <a href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a>,
              <i>Chelsea Finn</i><br>
              <em>Conference on Robot Learning (CoRL)</em>, 2021 <br>
              <a href="https://arxiv.org/abs/2109.10312">arXiv</a>  / <a href="https://sites.google.com/view/embr-site">video</a>
            </div>
            <div id="embr" style="display:none">  
              </div><br>

            <div onmouseover="document.getElementById('lorl').style.display = 'block';"
                onmouseout="document.getElementById('lorl').style.display='none';">
              <a href="https://arxiv.org/pdf/2109.01115.pdf">
                <papertitle>Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation</papertitle></a><br>
              <a href="http://surajnair.com">Suraj Nair</a>, 
              <a href="https://eric-mitchell.github.io/">Eric Mitchell</a>,
              <a href="https://cs.stanford.edu/~kchen92/">Kevin Chen</a>,
              <a href="https://brianichter.com/">Brian Ichter</a>, 
              <a href="https://cvgl.stanford.edu/silvio/">Silvio Savarese</a>, 
              <i>Chelsea Finn</i><br>
              <em>Conference on Robot Learning (CoRL)</em>, 2021 <br>
              <a href="https://arxiv.org/abs/2109.01115">arXiv</a>  / <a href="https://sites.google.com/view/robotlorl">project page</a>
            </div>
            <div id="lorl" style="display:none">  
              </div><br>

            <div onmouseover="document.getElementById('dvd').style.display = 'block';"
                onmouseout="document.getElementById('dvd').style.display='none';">
              <a href="https://arxiv.org/pdf/2103.16817.pdf">
                <papertitle>Learning Generalizable Robotic Reward Functions from "In-The-Wild" Human Videos</papertitle></a><br>
               <a href="https://anniesch.github.io/">Annie S. Chen</a>,
              <a href="http://surajnair.com">Suraj Nair</a>, 
              <i>Chelsea Finn</i><br>
              <em>Robotics: Science and Systems (RSS)</em>, 2021 <br>
              <a href="https://arxiv.org/abs/2103.16817">arXiv</a>  / <a href="https://sites.google.com/view/dvd-human-videos">project page</a>
            </div>
            <div id="dvd" style="display:none">  
              </div><br>

            <div onmouseover="document.getElementById('jtt').style.display = 'block';"
                onmouseout="document.getElementById('jtt').style.display='none';">
              <a href="https://arxiv.org/pdf/2107.09044.pdf">
                <papertitle>Just Train Twice: Improving Group Robustness without Training Group Information</papertitle></a><br>
               <a href="https://cs.stanford.edu/~evanliu/">Evan Z. Liu</a>*,
               <a href="https://www.linkedin.com/in/behzadhaghgoo/">Behzad Haghgoo</a>*,
               <a href="https://anniesch.github.io/">Annie S. Chen</a>*,
               <a href="https://stanford.edu/~aditir/">Aditi Raghunathan</a>,
               <a href="http://koh.pw/">Pang Wei Koh</a>,
               <a href="https://cs.stanford.edu/~ssagawa/">Shiori Sagawa</a>,
               <a href="https://cs.stanford.edu/~pliang/">Percy Liang</a>,
              <i>Chelsea Finn</i><br>
              <em>International Conference on Machine Learning (ICML)</em>, 2021 <font color="#e37222"><strong>(Long Talk)</strong></font>   <br>
              <a href="https://arxiv.org/abs/2107.09044">arXiv</a> 
            </div>
            <div id="jtt" style="display:none">  
              </div><br>

            <div onmouseover="document.getElementById('wilds').style.display = 'block';"
                onmouseout="document.getElementById('wilds').style.display='none';">
              <a href="https://arxiv.org/pdf/2012.07421.pdf">
                <papertitle>WILDS: A Benchmark of in-the-Wild Distribution Shifts </papertitle></a><br>
               <a href="http://koh.pw/">Pang Wei Koh</a>*,
               <a href="https://cs.stanford.edu/~ssagawa/">Shiori Sagawa</a>*,
              Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine,
              <i>Chelsea Finn</i>,
              Percy Liang
              <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2021 <font color="#e37222"><strong>(Long Talk)</strong></font>   <br>
              <a href="https://arxiv.org/abs/2012.07421">arXiv</a> / <a href="https://wilds.stanford.edu/">webpage</a> 
            </div>
            <div id="wilds" style="display:none">  
              </div><br>

            <div onmouseover="document.getElementById('dream').style.display = 'block';"
                onmouseout="document.getElementById('dream').style.display='none';">
              <a href="https://arxiv.org/pdf/2008.02790.pdf">
                <papertitle>Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices</papertitle></a><br>
               <a href="https://cs.stanford.edu/~evanliu/">Evan Z. Liu</a>,
               <a href="https://stanford.edu/~aditir/">Aditi Raghunathan</a>,
               <a href="https://cs.stanford.edu/~pliang/">Percy Liang</a>,
              <i>Chelsea Finn</i><br>
              <em>International Conference on Machine Learning (ICML)</em>, 2021 <br>
              <a href="https://arxiv.org/abs/2008.02790">arXiv</a> / <a href="https://ezliu.github.io/dream/">webpage</a> / <a href="https://www.youtube.com/watch?v=EiIC0Rkz8-s">talk</a> / <a href="https://ai.stanford.edu/blog/meta-exploration/">blog post</a> / <a href="https://github.com/ezliu/dream">code</a>
            </div>
            <div id="dream" style="display:none">  
              </div><br>

            <div onmouseover="document.getElementById('lompo').style.display = 'block';"
                onmouseout="document.getElementById('lompo').style.display='none';">
              <a href="https://arxiv.org/pdf/2012.11547.pdf">
                <papertitle>Offline Reinforcement Learning from Images with Latent Space Models </papertitle></a><br>
              <a href="https://deepai.org/profile/rafael-rafailov">Rafael Rafailov</a>*,
              <a href="https://cs.stanford.edu/~tianheyu/">Tianhe Yu</a>*,
              <a href="https://homes.cs.washington.edu/~aravraj/">Aravind Rajeswaran</a>,
              <i>Chelsea Finn</i><br>
              <em>Learning for Decision Making and Control (L4DC)</em>, 2021  <font color="#e37222"><strong>(Oral)</strong></font>   <br>
              <a href="https://arxiv.org/abs/2012.11547">arXiv</a> / <a href="https://sites.google.com/view/lompo/">webpage</a> 
            </div>
            <div id="lompo" style="display:none">  
              </div><br>

            <div onmouseover="document.getElementById('msr').style.display = 'block';"
                onmouseout="document.getElementById('msr').style.display='none';">
              <a href="https://arxiv.org/pdf/2007.02933.pdf">
                <papertitle>Meta-Learning Symmetries by Reparameterization</papertitle></a><br>
               <a href="http://bland.website/">Allan Zhou</a>,
               <a href="https://tomknowles.com/">Tom Knowles</a>,
              <i>Chelsea Finn</i><br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2021 <br>
              <a href="https://arxiv.org/abs/2007.02933">arXiv</a>  
            </div>
            <div id="msr" style="display:none">  
              </div><br>

            <div onmouseover="document.getElementById('rlv').style.display = 'block';"
                onmouseout="document.getElementById('rlv').style.display='none';">
              <a href="https://arxiv.org/pdf/2011.06507.pdf">
                <papertitle>Reinforcement Learning with Videos: Combining Offline Observations with Interaction </papertitle></a><br>
              <a href="https://sites.google.com/corp/view/karlschmeckpeper">Karl Schmeckpeper</a>,
              <a href="https://www.seas.upenn.edu/~oleh/">Oleh Rybkin</a>, 
              <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i>
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2020  <font color="#e37222"><strong>(Oral)</strong></font><br>
              <a href="https://arxiv.org/abs/2011.06507">arXiv</a> / <a href="https://sites.google.com/view/rl-with-videos">project page</a>
            </div>
              <div id="rlv" style="display:none">
              </div><br>

            <div onmouseover="document.getElementById('memo').style.display = 'block';"
                onmouseout="document.getElementById('memo').style.display='none';">
              <a href="https://arxiv.org/pdf/1912.03820.pdf">
                <papertitle>Meta-Learning without Memorization</papertitle></a><br>
              <a href="https://mingzhang-yin.github.io//">Mingzhang Yin</a>, 
              <a href="https://sites.google.com/corp/view/gjt/">George Tucker</a>, 
              <a href="https://mingyuanzhou.github.io//">Mingyuan Zhou</a>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2020  <font color="#e37222"><strong>(Spotlight)</strong></font> <br>
              <a href="https://arxiv.org/abs/1912.03820">arXiv</a>  / <a href="https://slideslive.com/38921876/bayesian-deep-learning-3">talk</a> / <a href="_files/neurips19_memorization.pdf">slides</a> / <a href="https://github.com/google-research/google-research/tree/master/meta_learning_without_memorization">code</a> 
            </div>
              <div id="memo" style="display:none">
              We identify and formally describe a peculiar, yet widespread problem with meta-learning algorithms that occurs from small and seemingly benign changes to the training set-up, and identify a meta-regularization solution for solving the problem for multiple classes of meta-learning methods.
              </div><br>

            <div onmouseover="document.getElementById('hvf').style.display = 'block';"
                onmouseout="document.getElementById('hvf').style.display='none';">
              <a href="https://arxiv.org/pdf/1909.05829.pdf">
                <papertitle>Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation</papertitle></a><br>
              <a href="http://surajnair.com">Suraj Nair</a>, 
              <i>Chelsea Finn</i>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2020  <br>
              <a href="https://arxiv.org/abs/1909.05829">arXiv</a>  / <a href="https://sites.google.com/stanford.edu/hvf">project page</a> / <a href="https://github.com/google-research/google-research/tree/master/hierarchical_foresight">code</a> 
            </div>
              <div id="hvf" style="display:none">
              We study how we can learn long-horizon vision-based tasks in self-supervised settings. Our approach, hierarchical visual foresight, can optimize for a sequence of subgoals that will make the task easier.
              </div><br>

            <div onmouseover="document.getElementById('robonet').style.display = 'block';"
                onmouseout="document.getElementById('robonet').style.display='none';">
              <a href="https://arxiv.org/pdf/1910.11215.pdf">
                <papertitle>RoboNet: Large-Scale Multi-Robot Learning</papertitle></a><br>
              <a href="https://sudeepdasari.github.io/">Sudeep Dasari</a>,
              <a href="https://febert.github.io/">Frederik Ebert</a>, 
              <a href="https://s-tian.github.io/">Stephen Tian</a>,
              <a href="http://surajnair.com">Suraj Nair</a>, 
              <a href="https://bucherb.github.io/">Bernadette Bucher</a>,
              <a href="https://sites.google.com/corp/view/karlschmeckpeper">Karl Schmeckpeper</a>,
              <a href="https://www.seas.upenn.edu/~sidsingh/">Siddharth Singh</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i><br>
              <em>Conference on Robot Learning (CoRL)</em>, 2019 <br>
              <a href="https://arxiv.org/abs/1910.11215">arXiv</a>  / <a href="https://www.robonet.wiki/">project page, code, data</a> / <a href="https://www.technologyreview.com/s/614668/welcome-to-robot-university-only-robots-need-apply/">press</a>
            </div>
            <div id="robonet" style="display:none">  
                The standard paradigm in robot learning is to set-up experiments in a single lab environment and train a robot from scratch from data collected in that setting. In contrast, essentially all machine learning fields accumulate and share large datasets across institutions, which enables training of models that generalize much more broadly. We aim to take a step in this direction by collecting a large dataset from 7 robot platforms across multiple institutions, which we call
                RoboNet. Critically, we find that pre-training on RoboNet enables us to generalize to entirely new robot platforms with less data than training from scratch.
              </div><br>

            <div onmouseover="document.getElementById('imaml').style.display = 'block';"
                onmouseout="document.getElementById('imaml').style.display='none';">
              <a href="https://arxiv.org/pdf/1909.04630.pdf">
                <papertitle>Meta-Learning with Implicit Gradients</papertitle></a><br>
              <a href="https://homes.cs.washington.edu/~aravraj/">Aravind Rajeswaran</a>*,
              <i>Chelsea Finn</i>*,
              <a href="https://homes.cs.washington.edu/~sham/">Sham Kakade</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a><br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2019 <br>
              <a href="https://arxiv.org/abs/1909.04630">arXiv</a>  / <a href="https://sites.google.com/view/imaml">project page</a>
            </div>
              <div id="imaml" style="display:none">
              Scaling meta-learning to long inner optimization procedures is difficult. We introduce iMAML, which meta-learns without differentiating through the inner optimization path using implicit differentiation. This allows you to use MAML with any inner loop optimizer. We also provide a theoretical analysis of the memory and computational requirements of a variety of meta-learning algorithms.
              </div><br>


            <div onmouseover="document.getElementById('hal').style.display = 'block';"
                onmouseout="document.getElementById('hal').style.display='none';">
              <a href="https://arxiv.org/pdf/1906.07343.pdf">
                <papertitle>Language as an Abstraction for Hierarchical Reinforcement Learning</papertitle></a><br>
              <a href="https://www.linkedin.com/in/yiding-jiang-600bb6116/">YiDing Jiang</a>,
              <a href="https://sites.google.com/corp/view/gugurus">Shixiang Gu</a>, 
              <a href="https://www.cs.ubc.ca/~murphyk/">Kevin Murphy</a>,
              <i>Chelsea Finn</i>
              <br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2019 <br>
              <a href="https://arxiv.org/abs/1906.07343">arXiv</a>  / <a href="https://sites.google.com/view/hal-demo">project page</a> / <a href="https://github.com/google-research/clevr_robot_env">code</a>
            </div>
              <div id="hal" style="display:none">
              We propose to use language as an abstraction for hierarchical reinforcement learning as it provides unique compositional structure, enabling fast learning and combinatorial generalization, while retaining tremendous flexibility, making it suitable for a variety of problems. We also introduce a new open-source environment inspired by the CLEVR dataset for studying language and interaction.
              </div><br>


            <div onmouseover="document.getElementById('gvf').style.display = 'block';"
                onmouseout="document.getElementById('gvf').style.display='none';">
              <a href="https://arxiv.org/pdf/1904.05538.pdf">
                <papertitle>Improvisation through Physical Understanding: Using Novel Objects as Tools with Visual Foresight</papertitle></a><br>
              <a href="https://anxie.github.io/">Annie Xie</a>,
              <a href="https://febert.github.io/">Frederik Ebert</a>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i>
              <br>
              <em>Robotics: Science and Systems (RSS)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1904.05538">arXiv</a> / <a href="https://sites.google.com/corp/view/gvf-tool">videos</a>
            </div>
              <div id="gvf" style="display:none">
                We study how robots can learn to use tools. With a combination of autonomous robot interaction (to learn about cause and effect) and teleoperated demonstrations (to learn about how to use tools), we show that robots can figure out how to solve tasks using novel tools and even improvise when conventional tools aren't available.
              </div><br>


            <div onmouseover="document.getElementById('pearl').style.display = 'block';"
                onmouseout="document.getElementById('pearl').style.display='none';">
              <a href="https://arxiv.org/pdf/1903.08254.pdf">
                <papertitle>Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables</papertitle></a><br>
              <a href="http://people.eecs.berkeley.edu/~rakelly">Kate Rakelly</a>*,
              <a href="https://scholar.google.com/citations?user=1O83J5MAAAAJ&hl=en/">Aurick Zhou</a>*,
              <a href="https://scholar.google.com/citations?user=eDQsOFMAAAAJ&hl=en/">Deirdre Quillen</a>,
              <i>Chelsea Finn</i>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>
              <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1903.08254">arXiv</a> / <a href="https://github.com/katerakelly/oyster">code</a>
            </div>
              <div id="pearl" style="display:none">
                We introduce PEARL, a method that leverages off-policy learning and a probabilistic belief over the task to make meta-reinforcement learning 20-100X more sample efficient.
              </div><br>

            <div onmouseover="document.getElementById('ladr').style.display = 'block';"
                onmouseout="document.getElementById('ladr').style.display='none';">
              <a href="https://arxiv.org/pdf/1803.11347.pdf">
                <papertitle>Learning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning</papertitle></a><br>
              <a href="https://people.eecs.berkeley.edu/~nagaban2/">Anusha Nagabandi*</a>, 
              <a href="https://iclavera.github.io/">Ignasi Clavera*</a>,
              Simin Liu,
              <a href="https://people.eecs.berkeley.edu/~ronf/">Ron Fearing</a>, 
              <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i><br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1803.11347">arXiv</a> / <a href="https://sites.google.com/view/learning2adapt">videos</a> / <a href="https://github.com/iclavera/learning_to_adapt">code</a>
              <br>
              <div id="ladr" style="display:none">
              We propose a method that learns how to adapt <i>online</i> to new situations and perturbations, through meta reinforcement learning. Unlike prior meta-RL methods,
              our approach is model-based, making it sample-efficient during meta-training and thus practical for real world problems.
              </div>
            </div><br>


            <div onmouseover="document.getElementById('uml').style.display = 'block';"
                onmouseout="document.getElementById('uml').style.display='none';">
              <a href="https://arxiv.org/pdf/1810.02334.pdf">
                <papertitle>Unsupervised Learning via Meta-Learning </papertitle></a><br>
              <a href="https://kylehsu.me/">Kyle Hsu</a>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i><br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1810.02334">arXiv</a> / <a href="https://sites.google.com/view/unsupervised-via-meta">project page</a> / <a href="https://github.com/hsukyle/cactus-maml">code</a>
              <br> 
              <div id="uml" style="display:none">
              We propose CACTUs, an unsupervised learning algorithm that learns to learn tasks constructed from unlabeled data. CACTUs leads to significantly more effective downstream learning and enables few-shot learning without requiring labeled meta-learning datasets.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('journalvf').style.display = 'block';"
                onmouseout="document.getElementById('journalvf').style.display='none';">
              <a href="https://arxiv.org/pdf/1812.00568.pdf">
                <papertitle>Visual Foresight: Model-Based Deep Reinforcement Learning for Vision-Based Robotic Control
</papertitle></a><br>
              <a href="https://febert.github.io/">Frederik Ebert</a>*, 
              <i>Chelsea Finn</i>*,
              <a href="https://sudeepdasari.github.io/">Sudeep Dasari</a>,
              <a href="https://anxie.github.io/">Annie Xie</a>,
              <a href="https://people.eecs.berkeley.edu/~alexlee_gk/">Alex Lee</a>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>
              <br>
              <a href="https://arxiv.org/abs/1812.00568">arXiv</a>  / <a href="https://sites.google.com/corp/view/visualforesight">project page</a> / <a href="https://github.com/SudeepDasari/visual_foresight">code</a> / <a href="https://sites.google.com/corp/berkeley.edu/robotic-interaction-datasets/home">data</a>
            </div>
              <div id="journalvf" style="display:none">
              We provide a unified overview of our work on visual foresight, and show new experiments that show how a single video prediction model can be used to solve many different vision-based tasks, including deformable object manipulation tasks involving towels, shorts, and shirts.
              </div><br>

            <div onmouseover="document.getElementById('thesis').style.display = 'block';"
                onmouseout="document.getElementById('thesis').style.display='none';">
              <a href="_files/dissertation.pdf">
                <papertitle>Learning to Learn with Gradients</papertitle></a><br>
              <i>Chelsea Finn</i><br>
              <em>PhD Dissertation</em>, 2018 <br>
              <div id="thesis" style="display:none">
              We develop a clear and formal definition of the meta-learning problem, its terminology, and desirable properties of meta-learning algorithms. Building upon these foundations, we present a class of model-agnostic meta-learning methods that embed gradient-based optimization into the learner. Finally, we show how these methods can be extended for applications in motor control by combining  elements of meta-learning with techniques for deep model-based reinforcement learning, imitation learning, and inverse reinforcement learning.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('daml').style.display = 'block';"
                onmouseout="document.getElementById('daml').style.display='none';">
              <a href="https://arxiv.org/pdf/1802.01557">
                <papertitle>One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning</papertitle></a><br>
              <a href="http://tianheyu927.github.io">Tianhe Yu</a>*,
              <i>Chelsea Finn</i>*,
              <a href="https://scholar.google.com/citations?user=lns9LUsAAAAJ&hl=en">Annie Xie</a>,
              Sudeep Dasari,
              <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
              <em>Robotics: Science and Systems (RSS)</em>, 2018  <br>
              <a href="https://arxiv.org/abs/1802.01557">arXiv</a> / <a href="https://sites.google.com/view/daml">video</a> / <a href="https://github.com/tianheyu927/mil">code</a> / <a href="http://bair.berkeley.edu/blog/2018/06/28/daml/">blog post</a>
              <br> 
              <div id="daml" style="display:none">
              We develop a domain-adaptive meta-learning method that allows for one-shot learning under domain shift. We show that our method can enable a robot to learn to maneuver a new object after seeing just
              one video of a human performing the task with that object.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('maml').style.display = 'block';"
                onmouseout="document.getElementById('maml').style.display='none';">
              <a href="https://arxiv.org/pdf/1703.03400.pdf">
                <papertitle>Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</papertitle></a><br>
              <i>Chelsea Finn</i>,
              <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2017 <br>
                <a href="https://arxiv.org/abs/1703.03400">arXiv</a>
                /
                <a href="http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/">blog post</a>
                /
                <a href="https://github.com/cbfinn/maml">code</a>
                /
                <a href="https://sites.google.com/view/maml">video results</a>
              <br> 
              <div id="maml" style="display:none">
              We propose a model-agnostic algorithm for meta-learning, where a model's parameters
              are trained such that a small number of gradient updates with a small amount of training data from a new task
              will produce good generalization performance on that task. Our method learns a classifier that can recognize
              images of new characters using only a few examples, and a policy that can rapidly adapt
              its behavior in simulated locomotion tasks.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('foresight').style.display = 'block';"
                onmouseout="document.getElementById('foresight').style.display='none';">
              <a href="https://arxiv.org/pdf/1610.00696.pdf">
                <papertitle>Deep Visual Foresight for Planning Robot Motion</papertitle></a><br>
              <i>Chelsea Finn</i>, <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2017 <br> 
              <strong style="color:#e37222">Best Cognitive Robotics Paper Finalist</strong><br>
                <a href="http://arxiv.org/abs/1610.00696">arXiv</a>
                /
                <a href="https://sites.google.com/site/robotforesight/">video</a>
              <br>
              <div id="foresight" style="display:none">
              We combine an action-conditioned predictive model of images, "visual foresight," with model-predictive control for planning how
              to push objects. The method is entirely self-supervised, requiring minimal human involvement.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('cdna').style.display = 'block';"
                onmouseout="document.getElementById('cdna').style.display='none';">
              <a href="https://arxiv.org/pdf/1605.07157.pdf">
                <papertitle>Unsupervised Learning for Physical Interaction through Video Prediction</papertitle></a><br>
              <i>Chelsea Finn</i>, <a href="http://goodfeli.github.io/">Ian Goodfellow</a>, <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
              <em>Neural Information Processing Systems (NIPS)</em>, 2016 <br> 
                <a href="http://arxiv.org/abs/1605.07157">arXiv</a>
                /
                <a href="https://sites.google.com/site/robotprediction/">videos</a>
                /
                <a href="https://sites.google.com/site/brainrobotdata/home/">data</a>
                /
                <a href="https://github.com/tensorflow/models/tree/master/research/video_prediction">code</a>
              <br>
              <div id="cdna" style="display:none">
              Our video prediction method predicts a transformation to apply to the previous image, rather than pixels values directly, leading to significantly improved multi-frame video prediction. We also introduce
              a dataset of 50,000 robotic pushing sequences, consisting of over 1 million frames.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('gcl').style.display = 'block';"
                onmouseout="document.getElementById('gcl').style.display='none';">
     <a href="http://jmlr.org/proceedings/papers/v48/finn16.pdf">
       <papertitle>Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization</papertitle></a><br>
     <i>Chelsea Finn</i>, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>, <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a><br>
     <em>International Conference on Machine Learning (ICML)</em>, 2016 <br>
     <strong style="color:#e37222">Oral presentation at the <a href="https://sites.google.com/site/nips2016deeplearnings/home">NIPS 2016 Deep Learning Symposium</a></strong><br>
     <a href="http://arxiv.org/abs/1603.00448">arXiv</a> /
     <a href="http://rll.berkeley.edu/gcl">video results</a> / 
     <a href="https://github.com/justinjfu/inverse_rl">code</a> /
     <a href="http://techtalks.tv/talks/guided-cost-learning-deep-inverse-optimal-control-via-policy-optimization/62472/">talk video</a>
     <br>
              <div id="gcl" style="display:none">
     We propose an method for Inverse Reinforcement Learning (IRL) that can handle unknown dynamics and scale to flexible, nonlinear cost functions. We evaluate our algorithm on a series of simulated tasks and real-world robotic manipulation problems, including pouring and inserting dishes into a rack.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('dsae').style.display = 'block';"
                onmouseout="document.getElementById('dsae').style.display='none';">
          
            <a href="http://arxiv.org/pdf/1509.06113.pdf">
              <papertitle>Deep Spatial Autoencoders for Visuomotor Learning</papertitle>
            </a><br>
            <i>Chelsea Finn</i>, <a href="https://sites.google.com/site/xinyutan17/">Xin Yu Tan</a>,
            <a href="http://www.rockyduan.com/">Yan Duan</a>, <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,
            <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a><br>
            <em>International Conference on Robotics and Automation (ICRA)</em>, 2016 <br>
            <a href="http://arxiv.org/abs/1509.06113">arXiv</a> /
            <a href="http://rll.berkeley.edu/dsae/">video</a>
          <br>
              <div id="dsae" style="display:none">
          We learn a lower dimensional visual state-space without supervision using deep spatial autoencoders, and use it to learn nonprehensile manipulation
          tasks, such as pushing a lego block and scooping a bag into a bowl.
              </div>
            </div><br>


            <div onmouseover="document.getElementById('e2e').style.display = 'block';"
                onmouseout="document.getElementById('e2e').style.display='none';">
              <a href="http://www.jmlr.org/papers/volume17/15-522/15-522.pdf">
                <papertitle>End-to-End Training of Deep Visuomotor Policies</papertitle></a><br>
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine*</a>,
              <i>Chelsea Finn</i>*, <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a><br>
              <strong style="color:#e37222">CCC Blue Sky Ideas <a href="http://www.cccblog.org/2015/08/03/blue-sky-ideas-aaai-rss-special-workshop-on-the-50th-anniversary-of-shakey/">Award</a></strong><br>
              <em>Journal of Machine Learning Research (JMLR)</em>, 2016 <br>
                <a href="https://arxiv.org/abs/1504.00702">arXiv</a> /
                <a href="https://sites.google.com/site/visuomotorpolicy/">video</a> /
                <a href="http://rll.berkeley.edu/deeplearningrobotics">project page</a> /
<a href="http://rll.berkeley.edu/gps">code</a>
              <br>
              <div id="e2e" style="display:none">
              We demonstrate a deep neural network trained end-to-end, from perception to controls, for robotic manipulation tasks.
              </div>
            </div><br>


        </td>
        -->

      </tbody></table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td>
        <br>
        <p align="right"><font size="2">
          <a href="https://people.eecs.berkeley.edu/~barron/">This guy makes a nice webpage.</a>
          </font>
        </p>
        </td>
      </tr>
  </tbody></table>

  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-59618557-1', 'auto');
                ga('send', 'pageview');

              </script>

    </td>
    </tr>
  </tbody></table>


</body></html>
